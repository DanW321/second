---
title: "Random Forest Training"
knit: (function(inputFile, encoding) { 
          rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file='html_files/rf_training.html') })
output:
  html_document:
    theme: default
    highlight: pygments
    toc: true
    toc_depth: 3
    toc_float: true
editor_options:
  chunk_output_type: console
---
Author: Dan Wexler\
Date: `r Sys.Date()`

This script walks through the process of training a random forest model that
will be used to label landscape disturbance patches in Mount Rainier, Olympic, 
and North Cascades National Parks. Below is a list of the inputs and outputs.\
\
INPUTS: (in '1_training_input' folder)\
1) .csv file generated by Google Earth Engine and ArcGIS Pro containing 
predictor variable information for disturbance polygons. (UPLOAD)\
OUTPUTS: (in '3_intermediate' folder)\
1) Trained random forest model that can be used to predict disturbance
labels for polygons.\
2) List of predictor variables used in the trained model.\
3) Table of model probabilities that is referenced in the labeling script.\
4) Table of omission error rates that is referenced in the labeling script.\

### (1) Setup
Here we load the R libraries that are used throughout the script. Some or all 
of them may have to be downloaded.
```{r setup 1, message = FALSE, warning = FALSE}
# loads R libraries
library(randomForest)
library(ggplot2)
library(tidyr)
library(caret)
library(dplyr)
library(DT)
library(here)
```

This block of code contains variables that the user can modify to fit their
specific needs. This is the only block of code that requires user input,
barring more involved modification of the script. The in-line comments detail
specifically what each variable represents.
```{r setup 2}
# the name of the park (MORA, OLYM, or NOCA)
park <- "MORA"
# the name of the file containing the predictor variables
predictors_file <- "predictors.csv"
# the subset of disturbances to include in this analysis (see the last block of
# code in this section to find what number corresponds to each disturbance)
disturbance_subset <- c(1, 2, 3, 4, 5, 7, 9, 16)
# if TRUE, filters out patches in the elevation/vegetation mask
filter_out_mask <- FALSE
# if TRUE, filters out patches from the year 1987
drop_1987 <- TRUE
# ---------------------------------------------------------------------------- #
# THE NEXT FOUR VARIABLES CONTROL WHICH PATCHES ARE INCLUDED IN THE TRAINING
# PROCESS AND WHICH ARE FILTERED OUT. SET ONE (AND ONLY ONE) TO TRUE. IF ALL
# FOUR ARE SET TO FALSE, THE ENTIRE STUDY AREA IS INCLUDED IN TRAINING.
# ---------------------------------------------------------------------------- #
# if TRUE, filters patches to those within the protected areas and buffer
model_pa_and_buffer <- FALSE
# if TRUE, filters patches to those within the park and buffer
model_park_and_buffer <- FALSE
# if TRUE, filters patches to those within the protected areas
model_pa <- FALSE
# if TRUE, filters patches to those within the park
model_park <- FALSE
# ---------------------------------------------------------------------------- #
# if TRUE, relabels 'post avalanche' disturbances as 'avalanche'
group_post_avalanche <- TRUE
# if TRUE, relabels 'post blowdown' disturbances as 'blowdown'
group_post_blowdown <- TRUE
# if TRUE, relabels 'post clearing' disturbances as 'clearing'
group_post_clearing <- TRUE
# if TRUE, relabels 'post fire' disturbances as 'fire'
group_post_fire <- TRUE
# if TRUE, relabels 'post mass movement' disturbances as 'mass movement'
group_post_mm <- TRUE
# if TRUE, relabels 'water' as 'annual variability'
group_water_with_av <- TRUE
# if TRUE, the testing set sizes of disturbances with ample data will be 
# restricted to better reflect the testing set sizes of disturbances with fewer 
# examples. This restriction is determine by the 'balance_multiplier' below
balance_testing_data <- FALSE
# the percent of data for each disturbance type to be used in model training
train_test_split <- 0.7
# how many times larger the training sets for more prevalent disturbances can
# be than the training set for the disturbance with the fewest examples. Also
# applied to testing set sizes if 'balance_testing_data' is TRUE
balance_multiplier <- 10
# the minimum distance in meters between disturbances of the same type in the 
# training set
min_train_distance <- 200
# the number of trees in the random forest model
num_trees <- 1000
# the number of variables to add each time a new random forest is trained
# when determining the optimal set of predictor variables
num_vars_add <- 5
# if TRUE, trains a random forest model without performing a train test split
# after probability cutoffs have been determined
train_all_data <- FALSE
```

This block of code contains a function for displaying tables that will be used
throughout the script.
```{r setup 4}
# function for displaying tables
create_table <- function(table, title, rows) {
  # displays the table
  datatable(table, class = "cell-border stripe hover",
            rownames = rows, caption = paste(park, "|", title),
            options = list(pageLength = nrow(table), dom = "t",
                         order = list(classes = "no-sort")))
}
```

Now we load the data. Throughout this script, 'x' will refer to the matrix
containing the predictor variables, and 'y' will refer the labels. Each
row in x corresponds to a disturbance patch, and each column is a predictor
variable. y is an array with labels for some or all of the disturbance patches
and also includes information about the location and timing of the disturbance.
This is used to filter the disturbance patches on which the model is trained.
```{r setup 5}
# loads predictor variables
x <- data.frame(read.csv(here(park, "1_training_input", predictors_file)))
# subsets predictor variable array
y <- subset(x, select = c("ChangeType", "In_Mask", "In_Buffer", "Protected", "In_Park"))
# loads list of predictor variables that may be used by the model
file_name <- paste0("starting_variables_", park, ".csv")
predictors <- read.csv(here(park, "2_starting_variables", file_name))$vars
```

This code creates two lists. The first is a list of the disturbance types,
which is used to filter the x and y arrays. The second is a list of shortened 
labels for the disturbance types, which is used for displaying tables and 
plots. Both lists are filtered by the disturbances chosen by the user in the 
first code block, using the 'disturbance_subset' variable. The numbers 
corresponding to each disturbance are also shown here. Finally, a frequency 
distribution of all labeled disturbances is displayed.
```{r setup 6}
# a list of disturbance types that is used to filter the data by disturbance
disturbances <- c("Annual Variability", # (1)
                  "Avalanche", # (2)
                  "Blowdown", # (3)
                  "Clearing", # (4)
                  "Defoliation", # (5)
                  "Development", # (6)
                  "Fire", # (7)
                  "Ice Damage", # (8)
                  "Mass Movement", # (9)
                  "Post Avalanche", # (10)
                  "Post Blowdown", # (11)
                  "Post Clearing", # (12)
                  "Post Defoliation", # (13)
                  "Post Fire", # (14)
                  "Post Mass Movement", # (15)
                  "Riparian Change", # (16)
                  "Water") # (17)
# a list of shortened disturbance labels used to display tables and plots
disturbance_labels <- c("AV", "Aval", "Blow", "Clear", "Defol", "Devel", 
                        "Fire", "Ice", "MM", "PosAval", "PosBlow", "PosClear", 
                        "PostDefol", "PosFire", "PostMM", "Ripar", "Water")
# filters the two lists above to include only the user-selected disturbances
disturbances <- disturbances[disturbance_subset]
disturbance_labels <- disturbance_labels[disturbance_subset]
# the number of selected disturbances
num_dists <- length(disturbances)
# displays a frequency distribution of all labeled disturbances
all_disturbances <- as.data.frame(table(y[y$ChangeType != "",]$ChangeType))
colnames(all_disturbances) <- c("Disturbance", "Frequency")
create_table(all_disturbances, "Frequency distribution of all disturbances", FALSE)
```

### (2) Filtering and Cleaning Data
Here, we clean and filter the x and y arrays. Some of these cleaning and
filtering processes are automatically performed, while others are only 
performed if the user sets certain variables to TRUE in the first code block. 
```{r filtering and cleaning data}
# removes non-selected columns from the x array
x <- subset(x, select = predictors)
# OPTIONAL: filters out patches within the elevation mask
mask_text <- "it was not"
if (filter_out_mask) {
  mask_text <- "it was"
  x_mask <- x[y$In_Mask == "Yes",]
  x_mask <- data.frame(sapply(x_mask, as.numeric))
  x_mask[is.na(x_mask)] <- 0
  x <- x[y$In_Mask != "Yes",]
  y <- y[y$In_Mask != "Yes",]
}
# converts predictor data to numeric and changes NA values to 0
x <- data.frame(sapply(x, as.numeric))
x[is.na(x)] <- 0
# OPTIONAL: filters out patches from the year 1987
if (drop_1987) {
  y <- y[x$yod != 1987,]
  x <- x[x$yod != 1987,]
}
# OPTIONAL: filters patches based on location in the study area
if (model_pa_and_buffer) {
  x <- x[which(y$Protected == "Yes" | y$In_Buffer == "Yes"),]
  y <- y[which(y$Protected == "Yes" | y$In_Buffer == "Yes"),]
} else if (model_park_and_buffer) {
  x <- x[which(y$In_Park == "Yes" | y$In_Buffer == "Yes"),]
  y <- y[which(y$In_Park == "Yes" | y$In_Buffer == "Yes"),]
} else if (model_pa) {
  x <- x[y$Protected == "Yes",]
  y <- y[y$Protected == "Yes",]
} else if (model_park) {
  x <- x[y$In_Park == "Yes",]
  y <- y[y$In_Park == "Yes",]
}
# extracts the disturbance labels from y
y <- data.frame(y$ChangeType)
# OPTIONAL: relabels 'post fire' as 'fire'
if (group_post_avalanche) {
  y[y[,1] == "Post Avalanche",] <- "Avalanche"
}
# OPTIONAL: relabels 'post blowdown' as 'blowdown'
if (group_post_blowdown) {
  y[y[,1] == "Post Blowdown",] <- "Blowdown"
}
# OPTIONAL: relabels 'post clearing' as 'clearing'
if (group_post_clearing) {
  y[y[,1] == "Post Clearing",] <- "Clearing"
}
# OPTIONAL: relabels 'post fire' as 'fire'
if (group_post_fire) {
  y[y[,1] == "Post Fire",] <- "Fire"
}
# OPTIONAL: relabels 'post mass movement' as 'mass movement'
if (group_post_mm) {
  y[y[,1] == "Post Mass Movement",] <- "Mass Movement"
}
# OPTIONAL: relabels 'water' as 'annual variability'
if (group_water_with_av) {
   y[y[,1] == "Water",] <- "Annual Variability"
}
# filters data by disturbance type
x <- x[y[,1] %in% disturbances,]
y <- y[y[,1] %in% disturbances,, drop = FALSE]
# OPTIONAL: modifies text based on whether a new model is going to be trained
section_text <- "(SKIP)"
if (train_all_data) {
  section_text <- ""
}
# displays a frequency distribution of all labeled disturbances after cleaning
all_disturbances <- as.data.frame(table(y))
colnames(all_disturbances) <- c("Disturbance", "Frequency")
title <- "Frequency distribution of disturbances after data clearning"
create_table(all_disturbances, title, FALSE)
```

### (3) Train-Test Split
An important component of many supervised machine learning models is a 
train-test split. The purpose of a train-test split is to purposefully withhold
some data from the training process so a model can be tested on unseen data 
later. It's relatively easy to build a machine learning model that 
performs well on data it has already seen before, but it's harder to build one 
that does well on unseen data. This section divides our data into a training 
set and a testing set.\
\
There are a few nuances to our specific case. The first is that, since
some disturbance types are much more frequent than others, we cannot
simply take a random sample of, say, 75% of all of our data and call this the
training set. It is very likely that some disturbances would not be represented
in either the training or testing set. To fix this issue, we perform stratified
sampling, which means we take a random subset of each disturbance type. This
ensures that each disturbance type will be represented. However, we would still
be dealing with extremely imbalanced data, as some disturbances have many more 
examples than other. These discrepancies would propagate into
the training and testing sets, and we don't want our model to only
perform well on the disturbances with the most examples. To fix this, we set
an upper limit on the number of examples of each disturbance that the model
can be trained on, which is `r balance_multiplier` times the number of examples 
present in the least-frequent disturbance class. Another nuance is that the 
user is able to specify, using the 'balance_testing_data' variable, whether the 
testing data is balanced like the training data. We will always train the 
random forest model with a relatively balanced data set, but it is up to the
user whether to test the model on a balanced data set. One advantage of not 
balancing the testing set is that, for disturbance classes with a large number 
of examples, we get a more representative classification accuracy. However, an 
unbalanced testing set will lead to less meaningful comission error rates. 
These are crucial considerations for assessing the model, but it is important 
to remember that balancing or not balancing the testing set will not affect the 
inherent performance of a specific random forest model.\
\
The final note is that, to reduce spatial autocorrelation, the training set is
constructed with a spatially diverse set of samples. In this run of the model,
this means that no two disturbances of the same type in the training set
are closer than `r min_train_distance ` meters to each other. This first block 
of code contains a function for calculating the distance between two disturbances.
```{r train-test split 1}
# a function that calculates the Euclidean distance between two disturbances
euclidean_distance <- function(dist1, dist2) {
  # converts from degrees to radians
  lat1 <- dist1$latitude * (pi / 180)
  lon1 <- dist1$longitude * (pi / 180)
  lat2 <- dist2$latitude * (pi / 180)
  lon2 <- dist2$longitude * (pi / 180)
  # the radius of the Earth
  R <- 6378100 
  # converts from spherical to Cartesian coordinates
  x1 <- R * cos(lat1) * cos(lon1)
  y1 <- R * cos(lat1) * sin(lon1)
  z1 <- R * sin(lat1)
  x2 <- R * cos(lat2) * cos(lon2)
  y2 <- R * cos(lat2) * sin(lon2)
  z2 <- R * sin(lat2)
  # calculates the Euclidean distance
  return(sqrt((x2 - x1)^2 + (y2 - y1)^2 + (z2 - z1)^2))
}
```

This code contains the algorithm for building the training and testing sets.
```{r train-test split 2}
# creates empty data frames that will store the training and testing sets
x_train <- data.frame()
y_train <- data.frame()
x_test <- data.frame()
y_test <- data.frame()
# the max number of disturbances to include in the training and testing sets
max_train_sample <- round(min(table(y)) * train_test_split, 0) * balance_multiplier
max_test_sample <- (min(table(y)) - round(min(table(y)) * train_test_split, 0)) * balance_multiplier
# loops through each disturbance type
for (disturbance in disturbances) {
  # filters the predictors and labels by disturbance type
  x_subset <- x[y[,1] == disturbance,]
  y_subset <- y[y[,1] == disturbance,, drop = FALSE]
  # finds the number of the current disturbance
  len_subset <- nrow(x_subset)
  # a vector for storing indices
  train_inds <- c()
  # a variable that will determine the size of the training set
  train_limit <- 0
  # if there are more disturbances than the allowed max, set the limit to the
  # max, otherwise set the limit to the number of disturbances * the split
  if (len_subset * train_test_split >= max_train_sample) {
    train_limit <- max_train_sample
  } else {
    train_limit <- round(len_subset * train_test_split, 0)
  }
  # a variable that will keep track of iterations
  counter <- 0
  # while the training set is not full and all of the disturbances of the
  # current type have not already been examined
  while (length(train_inds) < train_limit & counter < len_subset) {
    # picks a random index
    ind <- sample(setdiff(1:len_subset, train_inds), 1)
    # if the training set is not empty, loop through the set
    flag <- TRUE
    if (length(train_inds) > 0) {
      for (j in 1:length(train_inds)) {
        # if two disturbances are within a certain distance of each other, flip the flag
        if (euclidean_distance(x_subset[ind,], x_subset[train_inds[j],]) < min_train_distance) {
          flag <- FALSE
        }
      }
    }
    # if the flag is still TRUE, add the index to the training indices
    if (flag) {
      train_inds <- append(train_inds, ind)
    }
    # increment the counter
    counter <- counter + 1
  }
  # add disturbances to the training set
  x_train <- rbind(x_train, x_subset[train_inds,])
  y_train <- rbind(y_train, y_subset[train_inds,, drop = FALSE])
  # a variable that will determine the size of the testing set
  test_limit <- 0
  # if there are more remaining disturbances than the allowed max and
  # the 'balance_testing_data' variable is TRUE, set the limit to the max,
  # otherwise set the limit to the number of remaining disturbances
  if (length(setdiff(1:len_subset, train_inds)) >= max_test_sample & balance_testing_data) {
    test_limit <- max_test_sample
  } else {
    test_limit <- length(setdiff(1:len_subset, train_inds))
  }
  # generate a random sample of indices
  test_inds <- sample(setdiff(1:len_subset, train_inds), test_limit, replace = FALSE)
  # add disturbances to the test set
  x_test <- rbind(x_test, x_subset[test_inds,])
  y_test <- rbind(y_test, y_subset[test_inds,, drop = FALSE])
}
# checks to ensure that both the training and testing sets are populated
if (length(table(y_train)) != length(table(y_test))) {
  stop("ERROR: sample size of at least one disturbance too small for training")
}
# converts the labels to factors for compatibility with the random forest package
y_train <- factor(sapply(y_train, as.factor))
y_test <- factor(sapply(y_test, as.factor))
```

### (4) Random Forest Training
We are now ready to train a random forest model. This block of code trains five 
initial models using all of the available predictor variables and gets the
average importance for each variable across the models. This importance metric
is the mean decrease in model accuracy that would occur if a predictor was
removed from the model. A graph shows each variable's importance.
```{r random forest training 1, fig.align = "center", fig.height = ceiling(ncol(x_train) / 6)}
# the number of models to average over
importance_iterations <- 5
# the number of variables
num_vars_tot <- ncol(x_train)
# creates an empty vector to store importance results
importance <- vector("integer", num_vars_tot)
names(importance) <- colnames(x_train)
# iterates through a certain number of random forest models
for (i in 1:importance_iterations) {
  forest <- randomForest(x = x_train, y = y_train, importance = TRUE, ntree = num_trees)
  mda <- forest$importance[,"MeanDecreaseAccuracy"] / forest$importanceSD[,"MeanDecreaseAccuracy"]
  # adds the importance of each variable to the results vector
  importance <- importance + as.numeric(mda)
}
# averages the importance values across the models
importance <- sort(importance / importance_iterations, decreasing = TRUE)
# plots the importance values for each variable
to_plot <- data.frame(Variable = names(importance), Accuracy = unname(importance))
to_plot$Variable <- factor(to_plot$Variable, levels = to_plot$Variable[order(to_plot$Accuracy)])
ggplot(data = to_plot, aes(x = Variable, y = Accuracy)) + 
  geom_bar(stat = "identity", fill = "steelblue") + coord_flip() + 
  labs(y = "Mean Decrease in Accuracy", x = "", title = paste(park, "Variable Importance Ranking")) +
  theme(plot.title = element_text(hjust = 0.5))
```

This block of code contains a function that, given a confusion matrix,
calculates out of bag error and average class omission error.
```{r random forest training 2}
# a function that calculates out of bag and average omission error
confusion_matrix_stats <- function(confusion_matrix) {
  # three variables to store results
  omission <- 0
  correct <- 0
  total <- 0
  # loops through each disturbance type
  for (i in 1:num_dists) {
    row <- confusion_matrix[i,]
    # adds the omission error for a disturbance to the total
    if (sum(row) == 0) {
      omission <- omission + 1
    } else {
      omission <- omission + (1 - (row[i] / sum(row)))
    }
    # adds the number of correct classifications to the total
    correct <- correct + row[i]
    # adds the total number of classifications to the total
    total <- total + sum(row)
  }
  # calculates the omission error averaged across disturbance classes
  omission_return <- round((omission / num_dists) * 100, 2)
  # calculates the out of bag error
  oob_return <- round((1 - (correct / total)) * 100, 2)
  return (unname(c(omission_return, oob_return)))
}
```

Now that we have a ranked list of the most important variables, we can train
the random forest model. We start by training a model with the `r num_vars_add`
most important variables, set by the 'num_vars_add' variable. Successive models
are trained, adding `r num_vars_add` variables each time, and the model with
the lowest omission error, and the variables associated with that model, are
saved. This is the model that will be used to label unseen data. A graph
showing the out of bag error for each iteration is shown. The average omission 
error, the omission error for each disturbance class averaged by the number of 
classes, is also shown for reference.
```{r random forest training 3, fig.align = "center"}
# a variable for keeping track of the best model
min_err <- Inf
# a variable to store the best variables
best_vars <- NA
# a variable to store the best model
best_forest <- NA
# a data frame to store error results for plotting
results <- data.frame()
# a variable that tracks the number of variables to include each model
num_vars_inc <- num_vars_add
# sets the number of models that will be created
limit <- floor(num_vars_tot / num_vars_add)
if (num_vars_tot %% num_vars_add != 0) {
  limit <- limit + 1
}
# loops through a certain number of models
for (i in 1:limit) {
  # stores the number of variables in the current model
  results[i, 1] <- num_vars_inc
  # gets the variables to include in the current model
  vars <- names(importance[1:num_vars_inc])
  # subsets data by predictors
  x_train_subset <- x_train[,vars]
  x_test_subset <- x_test[,vars]
  # trains a new random forest using the current variables
  forest <- randomForest(x = x_train_subset, y = y_train, xtest = x_test_subset,
                         ytest = y_test, ntree = num_trees, keep.forest = TRUE)
  # stores the error results
  results[i, 2:3] <- confusion_matrix_stats(forest$confusion[,1:num_dists])
  err <- results[i, 3]
  # tracks the best random forest model
  if (err < min_err) {
    min_err <- err
    best_vars <- vars
    best_forest <- forest
  }
  # adjusts the number of variables to include in the next model
  num_vars_inc <- num_vars_inc + num_vars_add
  if (i == limit - 1) {
    num_vars_inc <- num_vars_tot
  }
}
# plots the out of bag and average omission errors for each iteration
ggplot(data = results, aes(x = V1)) + geom_line(aes(y = V2, color = "steelblue")) + 
  geom_line(aes(y = V3, color = "red")) +  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Number of Variables", y = "% Error", title = paste(park, "Random Forest Iterations")) +
  scale_color_discrete(name = "", labels = c("OOB", "Avg Omission")) + ylim(0, NA)
```

If the vegetation mask flag was set to TRUE (`r mask_text`), this block of code 
will display the results of the random forest labeling the patches in the mask 
that were withheld from the training and testing sets. Most of these labels 
should be 'Annual Variability'.
```{r random forest training 4}
# if the patches in the mask were withheld from training
if (filter_out_mask) {
  # use the forest to label the patches
  predictions <- as.data.frame(predict(forest, x_mask, type = "prob"))
  results <- data.frame(matrix(0, ncol(predictions), 1))
  rownames(results) <- colnames(predictions)
  colnames(results) <- c("Frequency")
  # analyze the predicted patches
  for (i in 1:nrow(predictions)) {
    ind <- which(predictions[i,] == max(predictions[i,]))
    results[ind, 1] <- results[ind, 1] + 1
  }
  # display the distribution of labels
  create_table(results, "Testing on Masked Patches", TRUE)
}
```

### (5) Random Forest Testing {.tabset .tabset-pills}
We will now evaluate the best random forest model created above (with the top
`r length(best_vars)` predictors). For each disturbance type, omission and 
comission error rates are calculated and displayed. The following chunk of 
code creates confusion matrices for the training and testing sets. Click the 
'Training' and 'Testing' buttons below to toggle between sets.
```{r random forest testing 1}
# a function that creates and formats a confusion matrix for display
create_confusion_matrix <- function(confusion) {
  # rounds omission error to two decimal points
  confusion[,"class.error"] <- round(confusion[,"class.error"] * 100, 2)
  # changes a column name
  colnames(confusion) <- append(disturbance_labels, "Omission Err (%)")
  # calculates the comission error for each disturbance type
  comissions <- list()
  for (i in 1:num_dists) {
    comission <- round((1 - (confusion[i, i] / sum(confusion[,i]))) * 100, 2)
    comissions <- append(comissions, comission)
  }
  # adds the comission error to the confusion matrix
  confusion <- rbind(confusion, append(comissions, NA))
  rownames(confusion) <- append(disturbance_labels, "Comission Err (%)")
  return (confusion)
}
# calls the function above and calculates average errors for the training set 
confusion_train <- create_confusion_matrix(best_forest$confusion)
omission_train <- round(mean(best_forest$confusion[,"class.error"]) * 100, 2)
comission_train <- round(mean(as.numeric(confusion_train["Comission Err (%)", 1:num_dists]), na.rm = TRUE), 2)
# calls the function above and calculates average errors for the testing set 
confusion_test <- create_confusion_matrix(best_forest$test$confusion)
omission_test <- round(mean(best_forest$test$confusion[,"class.error"]) * 100, 2)
comission_test <- round(mean(as.numeric(confusion_test["Comission Err (%)", 1:num_dists]), na.rm = TRUE), 2)
```

#### Training Results
This code displays the frequency distribution of the training set.
```{r random forest testing 3}
# displays a frequency distribution of the training set
train_table <- as.data.frame(table(y_train))
colnames(train_table) <- c("Disturbance", "Frequency")
create_table(train_table, "Frequency distribution of the training set", FALSE)
```

This code displays a confusion matrix for the training set. The rows represent 
real disturbance labels, and the columns represent the model's labels.
```{r random forest testing 4}
# displays a confusion matrix for the training set
caption <- paste("Training set confusion matrix (average omission error: ",
                 omission_train, "% | average comission error: ", comission_train, "%)", sep = "")
create_table(confusion_train, caption, TRUE)
```

#### Testing Results
This code displays the frequency distribution of the testing set.
```{r random forest testing 6}
# displays a frequency distribution for the testing set
test_table <- as.data.frame(table(y_test))
colnames(test_table) <- c("Disturbance", "Frequency")
create_table(test_table, "Frequency distribution of the testing set", FALSE)
```

This code displays a confusion matrix for the testing set. The rows represent 
real disturbance labels, and the columns represent the model's labels.
```{r random forest testing 7}
# displays a confusion matrix for the testing set
caption <- paste("Testing set confusion matrix (average omission error: ",
                 omission_test, "% | average comission error: ", comission_test, "%)", sep = "")
create_table(confusion_test, caption, TRUE)
```

### (6) Probability Evaluation {.tabset .tabset-pills}
When our random forest classifies a disturbance patch, it is taking the
majority vote among its `r num_trees` trees. For example, if 
`r round(num_trees / 2, 0)` out of `r num_trees` trees vote that a certain 
disturbance patch is an avalanche, then that patch is labeled as an avalanche 
(with more than two disturbance classes, only a plurality of votes is needed to
be the winning label). We will define the distribution of tree votes across the
different disturbance classes as the model probability. In other words, the 
patch described above has a 50% model probability of being an avalanche. It
is important to note that this model probability is not a true statistical
probability, rather a convenient way of describing the random forest's voting output.\
\
This section evaluates, for each type of disturbance, how the omission error
for the testing data changes as we threshold by model probability. Does the
omission error rate decrease as we look only at classifications made with
higher model probabilities? If this is the case, we can imagine setting a
model probability cutoff for each disturbance type that results in a lower
error rate, and then only accepting classifications made above that cutoff. 
However, a higher cutoff may lead to fewer disturbance patches being accepted. 
The purpose of this section is to provide the user with the necessary 
information to find the probability cutoff for each disturbance type that 
best balances omission error and the percent of patches that are classified.\
\
The following code calculates omission error rates and other
statistics for each disturbance type at different probability cutoffs.
```{r probability evaluation 1}
# the minimum number of disturbances of a certain class for statistics about
# that set of disturbances to be deemed relevant
min_num_dists <- 20
# extracts the forest's voting distribution over the testing set
votes <- best_forest$test$votes
# converts the testing labels to a data frame for easier analysis
y_test <- as.data.frame(y_test)
# extracts the forest's predictions for each disturbance patch
predicted <- as.data.frame(best_forest$test$predicted)
# creates a list of model probability cutoffs
probs <- seq(0, 1, 0.05)
# creates a list of nine error rates to examine
error_rates <- seq(5, 25, 2.5)
# creates four data frames to store results
result <- data.frame(matrix(0, length(probs), num_dists + 1))
cutoff <- data.frame(matrix(NA, num_dists, length(error_rates)))
percen <- data.frame(matrix(NA, num_dists, length(error_rates)))
errors <- data.frame(matrix(NA, num_dists, length(error_rates)))
# a list of column names for the data frames above
column_names <- c("< 5%", "< 7.5%", "< 10%", "< 12.5%", "< 15%", 
                  "< 17.5%", "< 20%", "< 22.5 %", "< 25%")
# loops through each disturbance type
for (i in 1:num_dists) {
  disturbance <- disturbances[i]
  # subsets the voting, prediction, and label arrays by the current disturbance
  vote_subset <- votes[y_test == disturbance,]
  pred_subset <- predicted[y_test == disturbance,, drop = FALSE]
  labl_subset <- y_test[y_test == disturbance,, drop = FALSE]
  # loops through the model probability cutoffs
  for (j in 1:length(probs)) {
    prob <- probs[j]
    numer <- 0
    denom <- 0
    # loops through each patch of the current disturbance type
    for (k in 1:nrow(vote_subset)) {
      # if the top disturbance type received a percentage of votes above
      # the current probability cutoff, increment the denominator
      if (max(vote_subset[k,]) >= prob) {
        denom <- denom + 1
        # if the model labeled the patch correctly, increment the numerator
        if (pred_subset[k, 1] == labl_subset[k, 1]) {
          numer <- numer + 1
        }
      }
    }
    # calculate the error for the current disturbance type and probability cutoff
    error <- round((1 - (numer / denom)) * 100, 2)
    result[j, i + 1] <- error
    # if if any correct classifications were made, loop through the error rates
    if (!is.na(error))  {
      for (l in 1:length(error_rates)) {
        # if the model error is lower than the current error, data
        # has not already been stored, and the error was calculated
        # using more than 20 data points, store the data
        if (error <= error_rates[l] & is.na(cutoff[i, l]) & denom >= min_num_dists) {
          cutoff[i, l] <- round(prob * 100, 0)
          percen[i, l] <- round(denom / test_table$Frequency[i] * 100, 2)
          errors[i, l] <- error
        }
      }
    }
  }
}
```

This chart shows the omission error for each disturbance class broken down by 
model probability cutoff. Blank spaces indicate that no classifications for a 
certain disturbance type were made above a certain cutoff. All values in percent.
```{r probability evaluation 2}
# displays the table described above
result[,1] <- round(probs * 100, 0)
colnames(result) <- append("Prob", disturbance_labels)
caption <- "Omission error for each disturbance at different model probability cutoffs"
create_table(result, caption, FALSE)
```

Use the information in the three tables below to determine, for each
disturbance type, what model probability cutoff best balances omission error
and the percent of patches that receive accepted labels. Note that these statistics
are only calculated when there are more than `r min_num_dists` examples of a
certain disturbance type available at a certain model probability cutoff. 
Statistics calculated with fewer than `r min_num_dists` disturbance examples 
are deemed not statistically relevant. Once you have determined a suitable
probability cutoff for each disturbance type, set those cutoffs in the
'rf_labeling.Rmd' script. Set the cutoff in the labelingscript to 'NA' if there 
is no suitable cutoff for that disturbance type. Also keep in mind that the 
omission error and percent of patches classified are statistics generated using 
the testing set, and therefore may not reflect the actual values that result 
when the model is applied to new, unseen data. Hopefully the testing set is 
representative, but there is no way to know for sure until a QAQC is performed 
on labeled data and actual error rates are determined.

#### Probability Cutoffs
This table shows the lowest model probability at which the random forest 
achieves below certain omission error rates for each disturbance class.
```{r probability evaluation 3}
# displays the table table above
cutoff <- cbind(disturbance_labels, cutoff)
colnames(cutoff) <- c("Error", column_names)
text <- "Probability Cutoffs (all values in %, only calculated when sample size >= "
title <- paste0(text, min_num_dists, ")")
create_table(cutoff, title, FALSE)
```

#### Percent Classified
This table shows what percent of the testing set for each disturbance type was 
classified at the probability cutoff shown in the 'Probability Cutoffs' table.
```{r probability evaluation 4}
# displays the table table above
percen <- cbind(disturbance_labels, percen)
colnames(percen) <- c("Error", column_names)
text <- "Percent Classified (all values in %, only calculated when sample size > "
title <- paste0(text, min_num_dists, ")")
create_table(percen, title, FALSE)
```

#### Omission Error Rates
This table shows the omission error rate across the testing set for each 
disturbance type at the probability cutoff shown in the 'Probability Cutoffs' table.
```{r probability evaluation 5}
# displays the table table above
errors <- cbind(disturbance_labels, errors)
colnames(errors) <- c("Error", column_names)
text <- "Omission Error Rates (all values in %, only calculated when sample size > "
title <- paste0(text, min_num_dists, ")")
create_table(errors, title, FALSE)
```

This block of code saves the model's labeling results for the testing set,
as well as the true labels, so "model_accuracy.R" can calculate the overall
classification error for a model.
```{r probability evaluation 6}
# saves the model's voting array
write.csv(data.frame(votes), file = here(park, "model_accuracy", "votes.csv"), row.names = FALSE)
# saves the model's predicted labels
colnames(predicted) <- c("predicted")
write.csv(data.frame(predicted), file = here(park, "model_accuracy", "predicted.csv"), row.names = FALSE)
# saves the true testing set labels
write.csv(data.frame(y_test), file = here(park, "model_accuracy", "y_test.csv"), row.names = FALSE)
```

### (7) Training New Model `r section_text`
```{r training new model 1, include = FALSE}
# text that will display if 'train_all_data' is TRUE
text_1 <- "If the 'train_all_data' variable was set to TRUE by the user, this
           section trains a new random forest without performing a train-test
           split. The reason for doing this is that disturbance types with
           few examples may be better classified by the model if the model is
           given more training examples with which to work. The assumption is
           that the model probability thresholds decided by the user will still
           be relevant to the new model, and can be applied to the labeling
           outputs. The hope is that the error estimates derived from the
           original thresholding would then be conservative, because the new
           model was trained on more data and may be more accurate. While it
           is generally not recommended to train a new model on all of the data
           and carry over assumptions and information from the old model, a
           QAQC will always be performed on labeling outputs and will indicate
           whether this retraining approach is valid."
text_2 <- "This code displays the frequency distribution of the training set."
text_3 <- "This code displays a confusion matrix for the training set. The rows 
           represent real disturbance labels, and the columns represent the 
           model's labels."
text_4 <- "This code displays a chart that shows the difference in omission error 
           rates between the new model and the original model for each disturbance 
           class. Negative values represent a decrease in error from the original
           model to the new model."
```

`r if (train_all_data) {text_1}`
```{r training new model 2, include = train_all_data, eval = train_all_data}
# creates empty data frames that will store the training set
x_train <- data.frame()
y_train <- data.frame()
# the max number of disturbances to include in the training set
max_train_sample <- min(table(y)) * balance_multiplier
# loops through each disturbance type
for (disturbance in disturbances) {
  # filters the predictors and labels by disturbance type
  x_subset <- x[y[,1] == disturbance,]
  y_subset <- y[y[,1] == disturbance,, drop = FALSE]
  # finds the number of the current disturbance
  len_subset <- nrow(x_subset)
  # a vector for storing indices
  train_inds <- c()
  # a variable that will determine the size of the training set
  train_limit <- 0
  # if there are more disturbances than the allowed max, set the limit to the
  # max, otherwise set the limit to the number of disturbances * the split
  if (len_subset >= max_train_sample) {
    train_limit <- max_train_sample
  } else {
    train_limit <- len_subset
  }
  # a variable that will keep track of iterations
  counter <- 0
  # while the training set is not full and all of the disturbances of the
  # current type have not already been examined
  while (length(train_inds) < train_limit & counter < len_subset) {
    # picks a random index
    ind <- sample(setdiff(1:len_subset, train_inds), 1)
    # if the training set is not empty, loop through the set
    flag <- TRUE
    if (length(train_inds) > 0) {
      for (j in 1:length(train_inds)) {
        # if two disturbances are within a certain distance of each other, flip the flag
        if (euclidean_distance(x_subset[ind,], x_subset[train_inds[j],]) < min_train_distance) {
          flag <- FALSE
        }
      }
    }
    # if the flag is still TRUE, add the index to the training indices
    if (flag) {
      train_inds <- append(train_inds,ind)
    }
    # increment the counter
    counter <- counter + 1
  }
  # adds disturbances to the training set
  x_train <- rbind(x_train, x_subset[train_inds,])
  y_train <- rbind(y_train, y_subset[train_inds,, drop = FALSE])
}
# converts the labels to factors for compatibility with the random forest package
y_train <- factor(sapply(y_train, as.factor))
# trains the new forest
best_forest <- randomForest(x = x_train, y = y_train, ntree = num_trees, keep.forest = TRUE)
confusion <- create_confusion_matrix(best_forest$confusion)
omission <- round(mean(best_forest$confusion[,"class.error"]) * 100, 2)
comission <- round(mean(as.numeric(confusion_train["Comission Err (%)", 1 : num_dists])), 2)
```

`r if (train_all_data) {text_2}`
```{r training new model 3, include = train_all_data, eval = train_all_data}
# displays a frequency distribution of the training set
train_table <- as.data.frame(table(y_train))
colnames(train_table) <- c("Disturbance", "Frequency")
create_table(train_table, "Frequency distribution of the training set", FALSE)
```

`r if (train_all_data) {text_3}`
```{r training new model 4, include = train_all_data, eval = train_all_data}
# displays a confusion matrix for the training set
caption <- paste("Training set confusion matrix (average omission error: ", 
                 omission, "% | average comission error: ", comission, "%)", sep = "")
create_table(confusion, caption, TRUE)
```

`r if (train_all_data) {text_4}`
```{r training new model 5, include = train_all_data, eval = train_all_data}
# gets the omission error rates for each model
cols <- ncol(confusion)
retrained_error <- as.numeric(confusion[1:num_dists, cols])
original_error <- as.numeric(confusion_train[1:num_dists, cols])
# creates a data frame to store results
difference <- data.frame(matrix(0, num_dists, 2))
# stores the disturbance type and error difference
difference[,1] <- disturbances
difference[,2] <- as.data.frame(round(retrained_error - original_error, 2))
# formats and displays the table
rownames(difference) <- disturbances
colnames(difference) <- c("Disturbance", "Error Difference")
create_table(difference, "Training Omission Error Difference (retrained - original)", FALSE)
```

### (8) Saving the Model
This block of code saves the trained random forest, the list of variables 
included in that model, and the model probability cutoffs and associated
omission error rates for use in the labeling script.
```{r saving the model}
# saves the variables used in the random forest model
file_name <- paste0("best_rf_variables_", park, ".csv")
write.csv(data.frame(best_vars), file = here(park, "3_intermediate", file_name), row.names = FALSE)
# saves the matrix with error results
file_name <- paste0("best_rf_errors_", park, ".csv")
write.csv(errors, file = here(park, "3_intermediate", file_name), row.names = FALSE)
# saves the matrix with the probability results
file_name <- paste0("best_rf_probabilities_", park, ".csv")
write.csv(cutoff, file = here(park, "3_intermediate", file_name), row.names = FALSE)
# saves the trained random forest model
file_name <- paste0("best_rf_model_", park, ".RData")
saveRDS(best_forest, file = here(park, "3_intermediate", file_name))
```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
